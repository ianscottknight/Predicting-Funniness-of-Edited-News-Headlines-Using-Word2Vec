{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Run the following in terminal in the virtual environment and restart the kernel before running this notebook:\n",
    "python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Imports from root dir are now possible:\n",
    "from src import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source:\n",
    "# https://www.kaggle.com/c/inls690-270-funny-news-headline/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 7239\n",
      "test set size: 2413\n",
      "<bound method NDFrame.head of          id                                           original           edit  \\\n",
      "0     10070  Lawmaker Who Assaulted Reporter Fights Court-O...        Shaving   \n",
      "1      1062  Trump rolls back Obama 's rule requiring emplo...           pets   \n",
      "2     12796  ' Who the hell is <Dana Rohrabacher/> ? ' Seth...         batman   \n",
      "3      1745  House Republicans just voted to gut the indepe...        laundry   \n",
      "4     13366  The Coca-Cola invasion is causing Mexico ’s sl...           mail   \n",
      "...     ...                                                ...            ...   \n",
      "7234  12642       Trump <looms/> over Georgia special election         stands   \n",
      "7235    100  Trump lawful group shake-up clears way for con...  disappearance   \n",
      "7236   3310  Trump will <pardon/> conservative pundit Dines...           date   \n",
      "7237   1518  Ancient ‘ frozen ’ tomb of Scythian <Prince/> ...         Scythe   \n",
      "7238  14471  Theresa May orders biggest expulsion of Russia...        wizards   \n",
      "\n",
      "      grades  meanGrade  \n",
      "0      22110        1.2  \n",
      "1      33100        1.4  \n",
      "2      11110        0.8  \n",
      "3      33200        1.6  \n",
      "4      21000        0.6  \n",
      "...      ...        ...  \n",
      "7234   10000        0.2  \n",
      "7235   11111        1.0  \n",
      "7236   32210        1.6  \n",
      "7237       0        0.0  \n",
      "7238   21110        1.0  \n",
      "\n",
      "[7239 rows x 5 columns]>\n",
      "<bound method NDFrame.head of          id                                           original         edit\n",
      "0     13109  Hillary Clinton warns LGBT progress may not be...       bridge\n",
      "1      3435  Gaza violence : Israel defends actions as 55 <...        newts\n",
      "2      3794  Germany ’s SPD Is Open to <Talks/> on New Merk...       fights\n",
      "3      8136  North Korea Signals Olympics Truce , Seeks <Ta...         rave\n",
      "4     11655  Trump On North Korea : ‘ We Have No Road Left ...       dinner\n",
      "...     ...                                                ...          ...\n",
      "2408   1931  ' The Trump slump ' : Remington files for bank...  Supersoaker\n",
      "2409  12142  Paul Ryan reportedly tells Trump the GOP lacks...      knowhow\n",
      "2410   1604  Trump 's ' Impenetrable ' <Cyber/> Unit That N...        Steak\n",
      "2411   4816  Young people are leaving the <Republican/> Par...       dinner\n",
      "2412  11353  Violent protests between Pro and Anti <Trump/>...      country\n",
      "\n",
      "[2413 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "train_path = f'{util.DATA_DIR}/train.csv'\n",
    "test_path = f'{util.DATA_DIR}/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print('training set size:', len(df_train))\n",
    "print('test set size:', len(df_test))\n",
    "\n",
    "print(df_train.head)\n",
    "print(df_test.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a71d268d7c486da5782298fbb3bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ef64456d1c4a5eb7ef07333d1bbe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2413.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"<\", \"\").replace(\"/>\", \"\") # remove hyphens\n",
    "    for w in text.split(\" \"):\n",
    "        if not w.isalpha():\n",
    "            text = text.replace(w, \"\")\n",
    "    text = \" \".join(text.split())\n",
    "    if all([w[0].isupper() for w in text.split(\" \") if w not in STOP_WORDS]):\n",
    "        text = text.lower()\n",
    "        text = text[0].upper() + text[1:]\n",
    "    text = text.replace(\"'\", \"\") # remove apostrophe (would cause problems later on) \n",
    "    return text\n",
    "\n",
    "# apply\n",
    "df_train[\"headline_preprocessed\"] = df_train[\"original\"].progress_apply(preprocess)\n",
    "df_test[\"headline_preprocessed\"] = df_test[\"original\"].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize replaced word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30693375523e4a88a4fc3c9a81270078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddc46b8457f48018fbecb79a9c5241c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2413.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_replaced_token(headline):\n",
    "    start = \"<\"\n",
    "    end = \"/>\"\n",
    "    replaced_token = headline[(headline.index(start)+len(start)):headline.index(end)].strip().lower()\n",
    "    return replaced_token\n",
    "\n",
    "df_train[\"replaced_token\"] = df_train[\"original\"].progress_apply(get_replaced_token)\n",
    "df_test[\"replaced_token\"] = df_test[\"original\"].progress_apply(get_replaced_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SpaCy pipeline with added custom component for recasting multi-word entites as single tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9901f039db4b15b669c904531cac44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503f86849a8343dab10e28005bd661f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2413.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# treat multi-word entities as individual tokens instead of multiple tokens (e.g. \"New York\" instead of \"New\" + \"York\")\n",
    "class EntityRetokenizeComponent:\n",
    "    def __init__(self, pipeline):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for ent in doc.ents:\n",
    "                retokenizer.merge(doc[ent.start:ent.end], attrs={\"LEMMA\": str(doc[ent.start:ent.end])})\n",
    "        return doc\n",
    "\n",
    "# create SpaCy pipeline\n",
    "spacy_pipeline = spacy.load(\"en_core_web_md\")\n",
    "retokenizer = EntityRetokenizeComponent(spacy_pipeline) \n",
    "spacy_pipeline.add_pipe(retokenizer, name='merge_enitities', last=True)\n",
    "\n",
    "# apply\n",
    "df_train[\"headline_spacy_obj\"] = df_train[\"headline_preprocessed\"].progress_apply(spacy_pipeline)\n",
    "df_test[\"headline_spacy_obj\"] = df_test[\"headline_preprocessed\"].progress_apply(spacy_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained Word2Vec model (GoogleNews, 300-dim)\n",
    "## Download here: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "## Unzip downloaded file and place in data directory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = f\"{util.DATA_DIR}/GoogleNews-vectors-negative300.bin\"\n",
    "w2v = word2vec.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(w2v.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize headlines using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee9b53969d6477c986faa4df3ef6a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb90094793e4cb6972de2c4509fdcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2413.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(spacy_obj):\n",
    "    tokens = []\n",
    "    for word in spacy_obj.doc:\n",
    "        w = str(word)\n",
    "        if spacy_pipeline.vocab[word.text.lower()].is_stop: continue\n",
    "        if w in vocab:\n",
    "            tokens.append(w)\n",
    "        else:\n",
    "            capitalized = \" \".join([x.capitalize() for x in w.split(\" \")])\n",
    "            if capitalized in vocab:\n",
    "                tokens.append(capitalized)\n",
    "            else:\n",
    "                w = w.lower()\n",
    "                if w in vocab:\n",
    "                    tokens.append(w)\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "df_train[\"headline_tokens\"] = df_train[\"headline_spacy_obj\"].progress_apply(tokenize)\n",
    "df_test[\"headline_tokens\"] = df_test[\"headline_spacy_obj\"].progress_apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize edit word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b904088069430f9e7cace045357131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d217d445b944da953210891147ddce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2413.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train[\"edit_token\"] = df_train[\"edit\"].progress_apply(lambda x: x.lower())\n",
    "df_test[\"edit_token\"] = df_test[\"edit\"].progress_apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features for supervised training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"meanGrade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average word vector of all the words in all headlines\n",
    "\n",
    "vecs = []\n",
    "for i, tokens in enumerate(df_train[\"headline_tokens\"]):\n",
    "    for token in tokens:\n",
    "        if token in w2v.vocab:\n",
    "            vec = w2v[token]\n",
    "            vecs.append(vec)\n",
    "avg_vec = np.nanmean(vecs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method: Subtract the word vector of the \n",
    "\n",
    "def get_X_diff(df):\n",
    "    X = np.zeros((len(df), 300))\n",
    "    for i, edit_token in enumerate(df[\"edit_token\"]):\n",
    "        replaced_token = df[\"replaced_token\"][i]\n",
    "        \n",
    "        if edit_token in w2v.vocab:\n",
    "            edit_vec = w2v[token]\n",
    "        else:\n",
    "            edit_vec = avg_vec\n",
    "            \n",
    "        if replaced_token in w2v.vocab:\n",
    "            replaced_vec = w2v[token]\n",
    "        else:\n",
    "            replaced_vec = avg_vec\n",
    "            \n",
    "        X[i,:] = edit_vec - replaced_vec\n",
    "        \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method: concatenate the three vectors: \n",
    "# (1) the average of the word vectors of the headline tokens\n",
    "# (2) the word vector of the replaced token\n",
    "# (3) the word vector of the edit token\n",
    "\n",
    "def get_X_concat(df):\n",
    "    feat_1 = np.zeros((len(df), 300))\n",
    "    feat_2 = np.zeros((len(df), 300))\n",
    "    feat_3 = np.zeros((len(df), 300))\n",
    "    \n",
    "    for i, tokens in enumerate(df[\"headline_tokens\"]):\n",
    "        vecs = []\n",
    "        for token in tokens:\n",
    "            if token in w2v.vocab:\n",
    "                vec = w2v[token]\n",
    "                vecs.append(vec)\n",
    "        if len(vecs) == 0:\n",
    "            vecs.append(np.zeros(300))\n",
    "        feat_1[i,:] = np.mean(vecs, axis=0)\n",
    "    \n",
    "    for i, token in enumerate(df[\"replaced_token\"]):\n",
    "        if token in w2v.vocab:\n",
    "            feat_2[i,:] = w2v[token]\n",
    "        else:\n",
    "            feat_2[i,:] = avg_vec\n",
    "        \n",
    "    for i, token in enumerate(df[\"edit_token\"]):\n",
    "        if token in w2v.vocab:\n",
    "            feat_3[i,:] = w2v[token]\n",
    "        else:\n",
    "            feat_3[i,:] = avg_vec\n",
    "            \n",
    "    X = np.concatenate((feat_1, feat_2, feat_3), axis=1)     \n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare validation errors of feature methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error of diff method: 0.34375664102877135\n"
     ]
    }
   ],
   "source": [
    "X_train_diff = get_X_diff(df_train)\n",
    "X_test_diff = get_X_diff(df_test)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_diff, y_train)\n",
    "print(f\"Validation error of diff method: {mean_squared_error(y_train, regressor.predict(X_train_diff))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error of concat method: 0.24446346473112185\n"
     ]
    }
   ],
   "source": [
    "X_train_concat = get_X_concat(df_train)\n",
    "X_test_concat = get_X_concat(df_test)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_concat, y_train)\n",
    "print(f\"Validation error of concat method: {mean_squared_error(y_train, regressor.predict(X_train_concat))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the feature vector produced by the concatenation method seems to provide a much lower validation error, even without parameter tuning, so we use it for as our feature vector for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_concat\n",
    "X_test = X_test_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search on various regressor models and their respective pools of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(regressor, params_dict, save_name):\n",
    "    gs = GridSearchCV(regressor, params_dict, cv=5)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(f\"Validation error: {mean_squared_error(y_train, gs.predict(X_train))}\")\n",
    "    print(\"Best parameters: \")\n",
    "    for key, value in gs.best_params_.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "        \n",
    "    y_pred = gs.predict(X_test)\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"id\": df_test[\"id\"],\n",
    "        \"pred\": y_pred\n",
    "    })\n",
    "    df_pred.to_csv(f\"{util.DATA_DIR}/{save_name}\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.24446346473112185\n",
      "Best parameters: \n",
      "\tnormalize: False\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "params_dict = {\n",
    "    \"normalize\": (False, True),\n",
    "}\n",
    "run_grid_search(regressor, params_dict, save_name=\"df_pred_lin_reg.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.34373150819691745\n",
      "Best parameters: \n",
      "\talpha: 0.1\n",
      "\tnormalize: False\n",
      "\ttol: 0.001\n"
     ]
    }
   ],
   "source": [
    "regressor = Lasso()\n",
    "params_dict = {\n",
    "    \"alpha\": (0.1, 0.5, 1.0, 2.0, 5.0),\n",
    "    \"normalize\": (False, True),\n",
    "    \"tol\": (1e-3, 1e-4, 1e-5),\n",
    "}\n",
    "run_grid_search(regressor, params_dict, save_name=\"df_pred_lasso.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.26516220346339964\n",
      "Best parameters: \n",
      "\talpha: 1.0\n",
      "\tnormalize: True\n",
      "\ttol: 0.001\n"
     ]
    }
   ],
   "source": [
    "regressor = Ridge()\n",
    "params_dict = {\n",
    "    \"alpha\": (0.1, 0.5, 1.0, 2.0, 5.0),\n",
    "    \"normalize\": (False, True),\n",
    "    \"tol\": (1e-3, 1e-4, 1e-5),\n",
    "}\n",
    "run_grid_search(regressor, params_dict, save_name=\"df_pred_ridge.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.24401159035048253\n",
      "Best parameters: \n",
      "\talpha: 0.0001\n",
      "\thidden_layer_sizes: (1000, 500, 200, 100, 100, 50, 10)\n",
      "\ttol: 0.0001\n"
     ]
    }
   ],
   "source": [
    "regressor = MLPRegressor(early_stopping=True)\n",
    "params_dict = {\n",
    "    \"hidden_layer_sizes\": ((1000,500,200,100,100,50,10),),\n",
    "    \"alpha\": (1.0e-4,),\n",
    "    \"tol\": (1.0e-4,)\n",
    "}\n",
    "run_grid_search(regressor, params_dict, save_name=\"df_pred_mlp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
